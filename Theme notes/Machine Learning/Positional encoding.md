После этапа [[embedding]], когда мы спроецировали слово в пространство более высокой размерности, нам необходимо как-то закодировать со словом порядок его появления в тексте. Это следующий этап трансформации -- [[Positional encoding]]. 

В идеале функция позиционного кодирования должна обладать следующими свойствами:
- Выходная позиция должна быть *уникальной* для каждой позиции (инъективность)
- Кодирование должно работать неизменно, как с *маленьким* текстом, так и с *большим*, расширение модели на б*о*льшие тексты должно происходить без особых трудозатрат и множество выходных значений должно быть *замкнутым*
- Для текста любого размера *расстояние* между любыми словами, расположенными на *постоянном* расстоянии, должно быть *одинаковым* или близким к *одинаковому* (равномерность)
- Модель должна быть *детерминированной*[[(?)]]


Вариант позиционного кодирования представлен в [[Position encoding -- вариант реализации]], в дополнение хочу заметить, что так как у нас M-мерное проективное пространство, то мы можем делать повороты в каждой плоскости (построенных на всевозможных парах координатных осей). 

[[LLM]] [[Positional encoding]] [[embedding]]