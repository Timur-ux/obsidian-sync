Процесс [[De-embedding]] можно воспринимать как обратный к [[embedding]], и если [[embedding]] реализован как в [[Идея реализации embedding]], то [[De-embedding]] реализуется простым транспонированием матрицы [[embedding]]'а. По сути мы просто меняем сторону прохода в нашей одноуровневой нейронной сети для [[embedding]], получая оную для [[De-embedding]]. 

Важно заметить, что результат [[De-embedding]]'а не будет выглядеть как one-hot-вектор. Однако искомое слово будет иметь максимальное значение равное 1. Близкие слова будут иметь значения вблизи 1, а остальные -- в районе 0 или меньше оного.

Используя функцию argmax, мы можем вернуть нашему вектору one-hot структуру.

Единственный недостаток функции argmax в том, что она выбирает слово с **наибольшим** значением, даже если разница этого слова со вторым по величине совсем незначительна.

Т.е. функция argmax подходит для строгого предугадывания запроса, однако если мы хотим какой-то вариативности в ответах, то данная функция нам не подходит. 

Для повышения вариативности ответов имеет смысл отойти от простых величин, к тому, чтобы свести их к вероятностям и, используя вероятностный анализ, выбирать следующее слово. Для этого существует даже специальный этап в процессе трансформации -- [[Softmax]]

[[LLM]] [[De-embedding]]