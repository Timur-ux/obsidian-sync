Здесь рассматривается вариант позиционного кодирования, представленный в научной работе от гугла: "Attention is all you need" от 2017 года.

Пусть $EM(w)$ -- функция [[embedding]]'а, $PE(t)$ -- позиционного кодирования, где
- $w$ -- слово
- $t$ -- номер слова

Положим, что размерность $PE(t)$ равна размерности $EM(w)$ и равна $d$.
Т.е. $PE(t)$ возвращает вектор, каждая компонента которого рассчитывается следующим образом:

$$PE(t)_{2i} = \sin(w_{2i}*t)$$
$$PE(t)_{2i+1} = \cos(w_{2i}*t)$$
где $w_k = 1/{10000^{(k/d)}}$ -- переменная частота. Заметим, что $k$ всегда четный и $d$ -- степень двойки([[Embedding -- вложение]]). Частота будет уменьшаться по мере увеличения $k$-й компоненты вектора. 

Т.к. длина волны по определению $$\lambda_k = \frac{2\pi}{w_k}$$ То длины волн образуют геометрическую прогрессию от $2*\pi$ до $10000*2*\pi$. Об этом упомянули в вышеупомянутой работе, кек. Пока не знаю, для чего это может быть полезно.

Итоговая позиция слова будет иметь вид
$$ENCODER(w, t) = EM(w) + PE(t)$$

[[LLM]] [[Positional encoding]]