Этот шаг в трансформации проходит каждый раз после определенного количества вычислений, таких как [[Multihead Self-attention]] или [[Feed-Forward]]. Используется для поддержания консистентности данных.

[[Transformer]] может легко забыть о предыдущих словах в запросе, заменив их на вероятности нахождения слов на данных позициях.

Так же присутствует проблема нелинейности нейронных сетей. Из-за этого при появлении больших значений в весах, [[Transformer]] может начать работать с большими сбоями.

Соответственно, с первой проблемой поможет справиться добавление входных слов в конец выхода -- [[Skip connections]], а со второй [[Normalization]]
