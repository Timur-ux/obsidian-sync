Вообще, [[Skip connections]] используется для сглаживания градиента, т.е. множество локальных минимумов, седел и т.п. сглаживаются и [[Backpropagation]] намного легче находит минимум целевой функции.

Не знаю общий алгоритм работы [[Skip connections]][[(?)]], но в [[LLM]] используется следующий вариант:
[[Self-attention]] является фильтром, и поэтому небольшие изменения во множестве входных слов могут быть пропущены, что не является хорошим знаком, поэтому копия входных слов добавляется к выходным словам, и, таким образом, небольшие изменения становятся значимыми, т.к. не забываются после множества этапом [[Multihead Self-attention]] и [[Feed-Forward]]

[[LLM]] [[Skip connections]]