Дословный перевод: *само-внимание* или *внимание на себя*.

Расширение демонстрационного примера [[Маска -- как внимание]], до реального механизма реализации внимания.

Основное отличие внимания от примитивного варианта, рассмотренного выше, состоит в том, что теперь в результате мы получаем не только 0 или 1 для предшествующих слов, а значение в интервале $[0, 1]$. Отход от дискретных значений имеет ряд плюсов, таких как возможность реализации вероятностного подхода, дифференцирования для применения обратного распространения и т.д.

В общем, мы можем для этого использовать уже описанную ранее функцию [[Softmax]].
Единственной его проблемой является то, что на выходе мы получаем единственное значение(слово), а нам было бы удобно также иметь на руках данные о нескольких предшествующих словах. Для решения этой проблемы используется [[Multihead Self-attention]]. 

Рассмотрим реализацию [[Self-attention]].

В математическом виде данная функция выглядит следующим образом:
$$
Attention(Q, K, V) = Softmax(\frac{Q\times K^T}{\sqrt{d_k}})V
$$
, где:
- $n$ -- максимальная длительность выходной последовательности [[LLM]]
- Q -- матрица запросов[[(?)]], размера $[n\times d_k]$
- K -- матрица ключей[[(?)]], размера $[n\times d_k]$
- V -- матрица значений[[(?)]], размера $[n\times d_v]$
*(смысл чисел $d_k$ и $d_v$ будет обоснован в [[Multihead Self-attention]])*

Интуитивное представление о смысле этого чуда таково: сначала матрицы $Q$ и $K^T$ перемножаются обычным матричным умножением. Получаем матрицу *(назовем её $P$)* размера $[n\times n]$. Она видимо задает то, как влияет каждое из $n$ слов на все $n$, к слову как те, что были раньше, так и те, что будут дальше, так и на данное слово.

*(В смысле внимания, наверное, имеет смысл только следующего прогнозируемого слова, т.к. те слова, что были раньше, уже выбраны и не будут меняться, а далекие будущие слова еще не выбраны, так как не выбраны промежуточные между ими и прогнозируемым слова. Тем не менее все эти данные могут опосредовано влиять на прогнозируемое слово, так что имеет смысл оставить их)*

Так вот, дальше матрица $P$ делится на $\sqrt{d_k}$, что *(из некоторых математических соображений)* приводит к тому, что значения в матрице $P$ не будут принимать запредельно большие значения.

Потом к полученной матрице $\frac{P}{\sqrt{d_k}}$ применяется функция [[Softmax]] и полученная матрица умножается на матрицу $V$. Получаем матрицу размера $[n\times d_v]$, которая *(интуитивно)* для каждой из $n$ позиций выдает вероятность нахождения на ней каждого из $d_k$ слов.

В качестве опционального действия можно после деления $P$  на $\sqrt{d_k}$  можно поставить в значения верхнего треугольника матрицы, не считая главную диагональ, минус бесконечность ($-\infty$). Это приведет к тому, что после применения функции [[Softmax]] вероятности данных позиций будут равны нулю, т.е. *прогнозирование не будет учитывать впередистоящие(будущие) слова*.

**Важным** замечанием является то, что данные вероятности зависят от матриц $Q, K$ и $V$,
структура которых определяет слово, на которое это самое внимание обращено, см. [[Multihead Self-attention]]


[[LLM]] [[Self-attention]] [[Multihead Self-attention]] [[Softmax]]