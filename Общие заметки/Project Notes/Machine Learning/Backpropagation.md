Оно же обратное распространение. Оно же обратное распространение ошибки. Метод машинного обучения, используемый для нахождения частных производных функции стоимости([[Loss]]) по каждому из параметров модели, которые впоследствии используются для построения градиента этой функции.

Идея метода заключается в следующем:
1. Вычисляем [[Loss]] с текущими параметрами модели. Это так называемый проход вперед ([[forward pass]]).
2. Дальше проходом назад([[Backpropagation]]) мы последовательно вычисляем частные производные по параметрам модели от конца к началу. Первичные частные производные высчитываются на выходе [[Loss]]'а, а следующие -- на основе уже вычисленных частных производных.
В дальнейшем мы можем использовать полученные данные для проведения оптимизационного шага([[Optimization step]])

В качестве примера рассмотрим алгоритм обратного распространения на примере построения прогнозирующей прямой через множество заданных точек на двумерной плоскости $Oxy$.
Постановка задачи:
Дано множество $M$ точек формата $(x_M, y_M)$, $|M| = n$. Необходимо подобрать такие параметры $\omega_1, \omega_2$, что прямая $y_g(x) = \omega_1x+\omega_2$ имеет минимальное расстояние до каждой из точек.

По сути нам надо минимизировать функцию стоимости 
$$C = \frac{1}{n}\sum_{i = 1}^n(y_{M_i} - y_g(x_{M_i}))^2 = \frac{1}{n}\sum_{i = 1}^nC_i$$  Для этого мы изначально можем задать $\omega_1, \omega_2$ произвольными числами. Потом посчитать $C$ с данными параметрами([[forward pass]])
Дальше будем искать $$\frac{\partial C}{\partial \omega_1}, \frac{\partial C}{\partial \omega_2},$$ используя [[Backpropagation]] по каждой тестовой паре, а дальше усредним полученные градиенты по всем парам. 
$$C_i = (y_{M_i} - y_g(x_{M_i}))^2$$
А дальше следите так сказать за цепочкой логических рассуждений(Идем в обратном порядке).
$$\frac{\partial C_i}{\partial C_i} = 1$$
$$\frac{\partial C_i}{\partial (\Delta y_i)} = 2\Delta y_i$$
$$\frac{\partial C_i}{\partial (y_g(x_{M_i}))} = \frac{\partial C_i}{\partial (\Delta y_i)}*\frac{\partial (\Delta y_i)}{\partial (y_g(x_{M_i}))} = 2\Delta y_i*(-1) = -2\Delta y_i$$
$$\frac{\partial C_i}{\partial (\omega_1)} = \frac{\partial C_i}{\partial (y_g(x_{M_i}))}\frac{\partial (y_g(x_{M_i}))}{\partial \omega_1} = -2\Delta y_i\frac{\partial (y_g(x_{M_i}))}{\partial \omega_1} = -2\Delta y_i\frac{\partial (\omega_1x_{M_i} + \omega_2)}{\partial \omega_1} = -2\Delta y_ix_{M_i}$$
$$\frac{\partial C_i}{\partial (\omega_2)} = \frac{\partial C_i}{\partial (y_g(x_{M_i}))}\frac{\partial (y_g(x_{M_i}))}{\partial \omega_2} = -2\Delta y_i\frac{\partial (y_g(x_{M_i}))}{\partial \omega_2} = -2\Delta y_i\frac{\partial (\omega_1x_{M_i} + \omega_2)}{\partial \omega_2} = -2\Delta y_i*1 = -2\Delta y_i$$

Да, для алгоритма обратного распространения используется цепное правило.
[[Machine learning]] [[Backpropagation]]