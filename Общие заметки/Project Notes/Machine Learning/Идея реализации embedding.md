Еmbedding можно проводить матричным умножением, например, one-hot encoded вектор можно умножить на матрицу размером NxM, и тем самым спроецировать N-мерный вектор в M-мерное пространство. Положение слова в M-мерном пространстве можно регулировать, меняя коэффициенты в матрице проецирования. Собственно, данные коэффициенты могут быть отрегулированы в процессе обучения.

Кстати, если воспринимать данную матрицу как матрицу весов нейронной сети с 1 входным, 1 выходным слоем(без промежуточных), то процесс обучения можно воспринимать буквально.

Ну и если embedding происходит, за счет использования матрицы, то довольно очевидным является то, как можно реализовать обратное преобразование -- [[De-embedding]]

[[LLM]]  [[embedding]]